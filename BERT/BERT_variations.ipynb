{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_variations.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%cd drive/MyDrive/Poems\n",
        "%ls"
      ],
      "metadata": {
        "id": "U23OEWaruTnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "FBYthz1DcEvF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hhgjc0swtX3f"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install sentence_transformers\n",
        "!pip install hazm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import torch\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from hazm import word_tokenize\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sentence_transformers import SentenceTransformer, CrossEncoder, util\n",
        "\n",
        "# def get_word_vector(sent, tokenizer, model, layers):\n",
        "#     \"\"\"Get a word vector by first tokenizing the input sentence, getting all token idxs\n",
        "#     that make up the word of interest, and then `get_hidden_states`.\"\"\"\n",
        "#     encoded = tokenizer.encode_plus(sent, return_tensors=\"pt\")\n",
        "#     with torch.no_grad():\n",
        "#         output = model(**encoded)\n",
        "#      # Get all hidden states\n",
        "#     states = output.hidden_states\n",
        "#      # Stack and sum all requested layers\n",
        "#     output = torch.stack([states[i] for i in layers]).sum(0).squeeze()\n",
        "#     # get all token idxs that belong to the word of interest\n",
        "#     word_nums = len(sent.split(\" \"))\n",
        "#     embeddings = []\n",
        "#     for idx in range(word_nums):\n",
        "#         token_ids_word = np.where(np.array(encoded.word_ids()) == idx)\n",
        "#         word_tokens_output = output[token_ids_word].mean(dim=0)\n",
        "#         embeddings.append(word_tokens_output)\n",
        "#     return embeddings\n",
        "\n",
        "# def get_query_similarity(query, tfidf, tfidf_array, wv_tfidf):\n",
        "#     query_vector = tfidf.transform([get_cleaned_text(query)]).toarray() @ wv_tfidf\n",
        "#     dot_product = tfidf_array * query_vector\n",
        "#     similarity = np.sum(dot_product, axis = 1)\n",
        "#     return similarity\n",
        "\n",
        "#########################################################################################################################\n",
        "\n",
        "def bert(query, num_results):\n",
        "    data = pd.read_csv('./bert_data/corpus.csv')\n",
        "    data = data.beyt.tolist()\n",
        "    \n",
        "    # encoder = SentenceTransformer('HooshvareLab/bert-fa-zwnj-base')\n",
        "    # encoder.save_pretrained('./bert_models/SimpleBert')\n",
        "    encoder = SentenceTransformer('./bert_models/SimpleBert')\n",
        "\n",
        "    # corpus_embeddings = encoder.encode(data, convert_to_tensor=True, show_progress_bar=True, batch_size=256)\n",
        "    # corpus_embeddings = corpus_embeddings.cpu().detach().numpy()\n",
        "    # np.save('./bert_data/simple_embeddings.npy', corpus_embeddings)\n",
        "    corpus_embeddings = torch.from_numpy(np.load('./bert_data/simple_embeddings.npy', allow_pickle=True))\n",
        "    \n",
        "    query_embedding = encoder.encode(query, convert_to_tensor=True).to('cpu')\n",
        "    cos_scores = util.pytorch_cos_sim(query_embedding, corpus_embeddings)[0, :]\n",
        "    cos_scores = cos_scores.cpu().detach().numpy()\n",
        "    most_relevant = np.argsort(cos_scores)[::-1]\n",
        "    results = []\n",
        "    for i in range(num_results):\n",
        "        results.append((most_relevant[i], data[most_relevant[i]]))\n",
        "    return results\n",
        "\n",
        "def bert_cross(query, num_results):\n",
        "    data = pd.read_csv('./bert_data/corpus.csv')\n",
        "    data = data.beyt.tolist()\n",
        "    encoder = SentenceTransformer('./bert_models/SimpleBert')\n",
        "    corpus_embeddings = torch.from_numpy(np.load('./bert_data/simple_embeddings.npy'))\n",
        "    query_embedding = encoder.encode(query, convert_to_tensor=True).to('cpu')\n",
        "    hits = util.semantic_search(query_embedding, corpus_embeddings, top_k=10)\n",
        "    hits = hits[0]\n",
        "    cross_encoder = CrossEncoder('./bert_models/CrossEncoder')\n",
        "    cross_inp = [[query, data[hit['corpus_id']]] for hit in hits]\n",
        "    cross_scores = cross_encoder.predict(cross_inp)\n",
        "    for idx in range(len(cross_scores)):\n",
        "        hits[idx]['cross-score'] = cross_scores[idx]\n",
        "    re_ranked = sorted(hits, key=lambda x: x['cross-score'], reverse=True)\n",
        "    results = []\n",
        "    for i in range(num_results):\n",
        "        results.append((re_ranked[i]['corpus_id'], data[re_ranked[i]['corpus_id']]))\n",
        "    return results\n",
        "\n",
        "def bert_finetuned(query, num_results):\n",
        "    data = pd.read_csv('./bert_data/corpus.csv')\n",
        "    data = data.beyt.tolist()\n",
        "    encoder = SentenceTransformer('./bert_models/MyBert')\n",
        "    \n",
        "    # corpus_embeddings = encoder.encode(data, convert_to_tensor=True, show_progress_bar=True, batch_size=256)\n",
        "    # corpus_embeddings = corpus_embeddings.cpu().detach().numpy()\n",
        "    # np.save('./bert_data/mybert_embeddings.npy', corpus_embeddings)\n",
        "    corpus_embeddings = torch.from_numpy(np.load('./bert_data/mybert_embeddings.npy'))\n",
        "    \n",
        "    query_embedding = encoder.encode(query, convert_to_tensor=True).to('cpu')\n",
        "    cos_scores = util.pytorch_cos_sim(query_embedding, corpus_embeddings)[0, :]\n",
        "    cos_scores = cos_scores.cpu().detach().numpy()\n",
        "    most_relevant = np.argsort(cos_scores)[::-1]\n",
        "    results = []\n",
        "    for i in range(num_results):\n",
        "        results.append((most_relevant[i], data[most_relevant[i]]))\n",
        "    return results\n",
        "\n",
        "# def bert_weighted_sum(query, num_results):\n",
        "#     data = pd.read_csv('./bert_data/corpus.csv')\n",
        "#     data = data.beyt.tolist()\n",
        "#     tokenizer = AutoTokenizer.from_pretrained('./bert_models/SimpleBert', local_files_only=True)\n",
        "#     # tokenizer = word_tokenize\n",
        "#     model = AutoModel.from_pretrained('./bert_models/SimpleBert', output_hidden_states=True, local_files_only=True)\n",
        "#     vectorizer_tfidf = TfidfVectorizer(norm='l2', tokenizer=word_tokenize, max_features=30000)\n",
        "\n",
        "#     layers = [-1]\n",
        "#     sent = \"ایشان هر روز ماهی میل می کردند .\"\n",
        "#     word_embedding = get_word_vector(sent, tokenizer, model, layers)\n",
        "#     print(len(word_embedding))\n",
        "#     print(word_embedding[0].shape)\n",
        "#     return\n",
        "    \n",
        "#     X = vectorizer_tfidf.fit_transform(data)\n",
        "#     with open('./bert_data/X', 'wb') as f:\n",
        "#         pickle.dump(X, f)\n",
        "#     # with open('./bert_data/X', 'rb') as f:\n",
        "#     #     X = pickle.load(f)\n",
        "#     print(type(X))\n",
        "#     print(X.shape)\n",
        "\n",
        "#     print(vectorizer_tfidf.get_feature_names())\n",
        "#     word_embedding_tfidf = get_word_vector(vectorizer_tfidf.get_feature_names(), tokenizer, model, layers)\n",
        "#     X_ft = X.toarray() @ word_embedding_tfidf\n",
        "#     doc_similarity = get_query_similarity(query, vectorizer_tfidf, X_ft, word_embedding_tfidf)\n",
        "#     relevance_order = np.argsort(doc_similarity)[::-1]\n",
        "#     results = []\n",
        "#     for i in range(num_results):\n",
        "#         results.append(data[relevance_order[i]])\n",
        "#     return results"
      ],
      "metadata": {
        "id": "YyYocB-2tcHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = 'خورشید و یار'\n",
        "k = 10\n",
        "\n",
        "# results = bert(query, k)\n",
        "# results = bert_cross(query, k)\n",
        "results = bert_finetuned(query, k)\n",
        "# results = bert_weighted_sum(query, k)\n",
        "\n",
        "results"
      ],
      "metadata": {
        "id": "Ws27w1-ntd0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluation**"
      ],
      "metadata": {
        "id": "2X9pCU748VEh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# rel_ids = {i+1:[] for i in range(100)}\n",
        "# data_pd = pd.read_csv('./bert_data/corpus.csv')\n",
        "# for qid in range(1, 100):\n",
        "#     print(qid, end=' ')\n",
        "#     with open('./evaluation_data/relevant/{}.txt'.format(qid), 'r') as f:\n",
        "#         lines = f.readlines()\n",
        "#         lines = [line[:-1] for line in lines]\n",
        "#     for line in lines:\n",
        "#         x = data_pd[data_pd['beyt'] == line].index.to_numpy()\n",
        "#         try:\n",
        "#             rel_ids[qid].append(x[0])\n",
        "#         except:\n",
        "#             pass\n",
        "#     # assert len(rel_ids[qid]) == len(lines)\n",
        "# rel_ids\n"
      ],
      "metadata": {
        "id": "DqW9fzayILPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with open('rel_ids.pickle', 'wb') as handle:\n",
        "#     pickle.dump(rel_ids, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "rel_ids = None\n",
        "with open('rel_ids.pickle', 'rb') as handle:\n",
        "    rel_ids = pickle.load(handle)"
      ],
      "metadata": {
        "id": "uDwwQ5muQRPK"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# queries_paths = './evaluation_data/eval_queries.txt'\n",
        "mutated_paths = './evaluation_data/eval_s.csv'\n",
        "ks = [1, 21, 41, 61, 81, 101]"
      ],
      "metadata": {
        "id": "7FPwR9PR9CQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def percision_at_k(rel, ret):\n",
        "    rel_set = set(rel)\n",
        "    ret_set = set(ret)\n",
        "    intersection = rel_set.intersection(ret_set)\n",
        "    return len(intersection) / len(ret)"
      ],
      "metadata": {
        "id": "yLnDPgqECspA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First and Third model\n",
        "\n",
        "encoder = SentenceTransformer('./bert_models/MyBert')\n",
        "corpus_embeddings = torch.from_numpy(np.load('./bert_data/mybert_embeddings.npy', allow_pickle=True)).to('cuda:0')\n",
        "\n",
        "mutated_queries = pd.read_csv(mutated_paths)\n",
        "data_pd = pd.read_csv('./bert_data/corpus.csv')\n",
        "data = data_pd.beyt.tolist()\n",
        "\n",
        "for k in ks:\n",
        "    pers = []\n",
        "    for i, row in mutated_queries.iterrows():\n",
        "        q = row['text']\n",
        "        qid = int(row['original_query_id'])\n",
        "\n",
        "        query_embedding = encoder.encode(q, convert_to_tensor=True)\n",
        "        cos_scores = util.pytorch_cos_sim(query_embedding, corpus_embeddings)[0, :]\n",
        "        cos_scores = cos_scores.cpu().detach().numpy()\n",
        "        most_relevant = np.argsort(cos_scores)[::-1]\n",
        "        results = []\n",
        "        for j in range(k):\n",
        "            results.append((most_relevant[j], data[most_relevant[j]]))\n",
        "\n",
        "        returned_ids = [j for j, _ in results]\n",
        "        \n",
        "        rids = rel_ids[qid]\n",
        "        # with open('./evaluation_data/relevant/{}.txt'.format(qid), 'r') as f:\n",
        "        #     lines = f.readlines()\n",
        "        #     lines = [line[:-1] for line in lines]\n",
        "        # for line in lines:\n",
        "        #     x = data_pd[data_pd['beyt'] == line].index.to_numpy()\n",
        "        #     rel_ids.append(x[0])\n",
        "        # assert len(rel_ids) == len(lines)\n",
        "\n",
        "        p_at_k = percision_at_k(rids, returned_ids)\n",
        "        pers.append(p_at_k)\n",
        "\n",
        "    print('Precision at {}:'.format(k), sum(pers) / len(pers))"
      ],
      "metadata": {
        "id": "_ioyKto08T2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Second model\n",
        "\n",
        "encoder = SentenceTransformer('./bert_models/SimpleBert')\n",
        "cross_encoder = CrossEncoder('./bert_models/CrossEncoder')\n",
        "corpus_embeddings = torch.from_numpy(np.load('./bert_data/simple_embeddings.npy', allow_pickle=True)).to('cuda:0')\n",
        "\n",
        "mutated_queries = pd.read_csv(mutated_paths)\n",
        "data_pd = pd.read_csv('./bert_data/corpus.csv')\n",
        "data = data_pd.beyt.tolist()\n",
        "\n",
        "for k in ks:\n",
        "    pers = []\n",
        "    for i, row in mutated_queries.iterrows():\n",
        "        # print(i, end=' ')\n",
        "        q = row['text']\n",
        "        qid = int(row['original_query_id'])\n",
        "\n",
        "        query_embedding = encoder.encode(q, convert_to_tensor=True)\n",
        "        hits = util.semantic_search(query_embedding, corpus_embeddings, top_k=100)\n",
        "        hits = hits[0]\n",
        "        cross_inp = [[q, data[hit['corpus_id']]] for hit in hits]\n",
        "        cross_scores = cross_encoder.predict(cross_inp)\n",
        "        for idx in range(len(cross_scores)):\n",
        "            hits[idx]['cross-score'] = cross_scores[idx]\n",
        "        re_ranked = sorted(hits, key=lambda x: x['cross-score'], reverse=True)\n",
        "        most_relevant = np.argsort(cos_scores)[::-1]\n",
        "        results = []\n",
        "        for j in range(k):\n",
        "            results.append((most_relevant[j], data[most_relevant[j]]))\n",
        "\n",
        "        returned_ids = [j for j, _ in results]\n",
        "        \n",
        "        rids = rel_ids[qid]\n",
        "        # with open('./evaluation_data/relevant/{}.txt'.format(qid), 'r') as f:\n",
        "        #     lines = f.readlines()\n",
        "        #     lines = [line[:-1] for line in lines]\n",
        "        # for line in lines:\n",
        "        #     x = data_pd[data_pd['beyt'] == line].index.to_numpy()\n",
        "        #     rel_ids.append(x[0])\n",
        "        # assert len(rel_ids) == len(lines)\n",
        "\n",
        "        p_at_k = percision_at_k(rids, returned_ids)\n",
        "        pers.append(p_at_k)\n",
        "\n",
        "    print('Precision at {}:'.format(k), sum(pers) / len(pers))"
      ],
      "metadata": {
        "id": "CR-i3xEkN-Tl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Temp**"
      ],
      "metadata": {
        "id": "DAHRNoNG_GDF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = AutoModel.from_pretrained('HooshvareLab/bert-fa-zwnj-base')\n",
        "tokenizer = AutoTokenizer.from_pretrained('HooshvareLab/bert-fa-zwnj-base')\n",
        "encoder.save_pretrained('./bert_models/SimpleBert')\n",
        "tokenizer.save_pretrained('./bert_models/SimpleBert')\n",
        "tokenizer.save_pretrained('./bert_models/MyBert')"
      ],
      "metadata": {
        "id": "yqDq4EomtfU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = AutoModel.from_pretrained(\"m3hrdadfi/bert-fa-base-uncased-wikinli\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"m3hrdadfi/bert-fa-base-uncased-wikinli\")\n",
        "encoder.save_pretrained('./bert_models/CrossEncoder')\n",
        "tokenizer.save_pretrained('./bert_models/CrossEncoder')"
      ],
      "metadata": {
        "id": "peqxomI53ig8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l = []\n",
        "with open('./evaluation_data/eval_queries.txt', 'r') as f:\n",
        "    lines = f.readlines()\n",
        "    lines = [s[:-1] for s in lines]\n",
        "    for i, line in enumerate(lines):\n",
        "        l.append({'original_query_id':i+1, 'text': line})\n",
        "df = pd.DataFrame(l)\n",
        "df.to_csv('./evaluation_data/eval_s.csv')"
      ],
      "metadata": {
        "id": "tK6m0x8P_20F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Fb7eqSo7QdlW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}