{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Nlp-Project-Evaluation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COBzJ5aCuDv4",
        "outputId": "a99054e7-6f1f-4fc4-dd29-cd6be9d06c3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd drive/MyDrive/NLP/Project"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmgldONDupuS",
        "outputId": "554a36bc-08e4-42f3-ba09-e628875b8072"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/NLP/Project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Required Packages"
      ],
      "metadata": {
        "id": "hntL3QkTvtzZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install hazm\n",
        "!pip install tokenizers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4l82QENHuqx5",
        "outputId": "0b720309-f295-4f22-c35e-3859a068e9d9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting hazm\n",
            "  Downloading hazm-0.7.0-py3-none-any.whl (316 kB)\n",
            "\u001b[K     |████████████████████████████████| 316 kB 5.2 MB/s \n",
            "\u001b[?25hCollecting nltk==3.3\n",
            "  Downloading nltk-3.3.0.zip (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 55.5 MB/s \n",
            "\u001b[?25hCollecting libwapiti>=0.2.1\n",
            "  Downloading libwapiti-0.2.1.tar.gz (233 kB)\n",
            "\u001b[K     |████████████████████████████████| 233 kB 62.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.3->hazm) (1.15.0)\n",
            "Building wheels for collected packages: nltk, libwapiti\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.3-py3-none-any.whl size=1394488 sha256=bb05f8fd46fe4fe0f3e7df0475501c18809233e4eab95ec2377c06dfead7b3a3\n",
            "  Stored in directory: /root/.cache/pip/wheels/9b/fd/0c/d92302c876e5de87ebd7fc0979d82edb93e2d8d768bf71fac4\n",
            "  Building wheel for libwapiti (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for libwapiti: filename=libwapiti-0.2.1-cp37-cp37m-linux_x86_64.whl size=154784 sha256=04555dc81726f72941125f79e7e6f708d0a3f7fd24896185042016ec2acb4342\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/b2/5b/0fe4b8f5c0e65341e8ea7bb3f4a6ebabfe8b1ac31322392dbf\n",
            "Successfully built nltk libwapiti\n",
            "Installing collected packages: nltk, libwapiti, hazm\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed hazm-0.7.0 libwapiti-0.2.1 nltk-3.3\n",
            "Collecting tokenizers\n",
            "  Downloading tokenizers-0.11.5-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8 MB 5.1 MB/s \n",
            "\u001b[?25hInstalling collected packages: tokenizers\n",
            "Successfully installed tokenizers-0.11.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Required Packages"
      ],
      "metadata": {
        "id": "6xA7u8w-vwKo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "from hazm import *\n",
        "import codecs\n",
        "import tqdm\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import yaml\n",
        "import glob\n",
        "import linecache\n",
        "import matplotlib.pyplot as plt\n",
        "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers, processors\n",
        "from gensim.models import FastText\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "\n",
        "normalizer = Normalizer()\n",
        "stemmer = Stemmer()\n",
        "lemmatizer = Lemmatizer()\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "rvcYyhzSvk7A"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read data"
      ],
      "metadata": {
        "id": "bgMoihPlv1ln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "poets = glob.glob('./*.txt')\n",
        "poems = []\n",
        "\n",
        "for poem_file in poets:\n",
        "    with open(poem_file, encoding='utf-8', mode='r') as fp:\n",
        "        line = fp.readline()\n",
        "        cnt = 1\n",
        "        box = ''\n",
        "        while line:\n",
        "            if line.strip() != '':\n",
        "                box = box + ' ' + line.strip()\n",
        "                if cnt % 2 == 0:\n",
        "                    poems.append(box.strip())\n",
        "                    box = ''\n",
        "                cnt += 1\n",
        "            line = fp.readline()"
      ],
      "metadata": {
        "id": "iguic25yvrsK"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "queries = []\n",
        "query_indexes = []\n",
        "queries1_matches = glob.glob('./Evaluation/queries1/retrieved/*.txt')\n",
        "\n",
        "for i in range(1, 51):\n",
        "    with open('./Evaluation/queries1/retrieved/' + str(i) + '.txt', encoding='utf-8', mode='r') as fp:\n",
        "        indexes = []\n",
        "        line = fp.readline()\n",
        "        while line:\n",
        "            if line.strip() != '':\n",
        "                try:\n",
        "                    index = poems.index(line.strip())\n",
        "                    indexes.append(index)\n",
        "                except Exception:\n",
        "                    pass\n",
        "            line = fp.readline()\n",
        "    if len(indexes):\n",
        "        query_indexes.append(indexes)\n",
        "        query = linecache.getline('./Evaluation/queries1/queries.txt', i)\n",
        "        queries.append(query)\n",
        "\n",
        "\n",
        "\n",
        "queries2_matches = glob.glob('./Evaluation/queries2/retrieved/*.txt')\n",
        "\n",
        "for i in range(1, 51):\n",
        "    with open('./Evaluation/queries2/retrieved/' + str(i) + '.txt', encoding='utf-8', mode='r') as fp:\n",
        "        indexes = []\n",
        "        line = fp.readline()\n",
        "        while line:\n",
        "            if line.strip() != '':\n",
        "                try:\n",
        "                    index = poems.index(line.strip())\n",
        "                    indexes.append(index)\n",
        "                except Exception:\n",
        "                    pass\n",
        "            line = fp.readline()\n",
        "    if len(indexes):\n",
        "        query_indexes.append(indexes)\n",
        "        query = linecache.getline('./Evaluation/queries2/queries.txt', i).strip()\n",
        "        queries.append(query)"
      ],
      "metadata": {
        "id": "uU4QqxZ82C6H"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def MMR(model_path):\n",
        "    mrr_values = []\n",
        "    for ind, query in enumerate(tqdm.tqdm(queries)):\n",
        "        model = Doc2Vec.load(model_path)\n",
        "        tokens = word_tokenize(query)\n",
        "        new_vector = model.infer_vector(doc_words=tokens)\n",
        "        similarity = model.docvecs.most_similar([new_vector], topn = len(poems))\n",
        "        similarity_indexes = []\n",
        "        for sim in similarity:\n",
        "            similarity_indexes.append(int(sim[0]))\n",
        "        mrr_value = 0\n",
        "        for query_index in query_indexes[ind]:\n",
        "            mrr_value += 1/(similarity_indexes.index(query_index) + 1)\n",
        "        mrr_values.append(mrr_value / len(query_indexes[ind]))\n",
        "\n",
        "    return mrr_values\n",
        "\n",
        "\n",
        "def evaluate_model(model_path, precisions_at_k_values):\n",
        "    mrr_values = []\n",
        "    precision_at_k = np.zeros(len(precisions_at_k_values))\n",
        "    for ind, query in enumerate(tqdm.tqdm(queries)):\n",
        "        model = Doc2Vec.load(model_path)\n",
        "        tokens = word_tokenize(query)\n",
        "        new_vector = model.infer_vector(doc_words=tokens)\n",
        "        similarity = model.docvecs.most_similar([new_vector], topn = len(poems))\n",
        "        similarity_indexes = []\n",
        "        for sim in similarity:\n",
        "            similarity_indexes.append(int(sim[0]))\n",
        "        mrr_value = 0\n",
        "        for query_index in query_indexes[ind]:\n",
        "            mrr_value += 1/(similarity_indexes.index(query_index) + 1)\n",
        "        mrr_values.append(mrr_value / len(query_indexes[ind]))\n",
        "\n",
        "        for ind_pak, precisions_at_k_value in enumerate(precisions_at_k_values):\n",
        "            ground_truth = set(query_indexes[ind])\n",
        "            preds = set(similarity_indexes[:precisions_at_k_value])\n",
        "            intersection = ground_truth.intersection(preds)\n",
        "            precision_at_k[ind_pak] += len(intersection) / min(precisions_at_k_value, len(ground_truth))\n",
        "\n",
        "    precision_at_k /= len(queries)        \n",
        "    return mrr_values, precision_at_k"
      ],
      "metadata": {
        "id": "zO47eTfrCEeJ"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dbow_300_mrr, dbow_300_pak = evaluate_model('./Models/PV_DBOW_vec300', [20, 40, 60, 80, 100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8JSFvsEUEaNf",
        "outputId": "fc1b1c43-cd6a-412f-f901-e5bc8b009739"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 99/99 [11:34<00:00,  7.01s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('DBOW with vector size 300 results:')\n",
        "print('MRR: {}'.format(np.mean(dbow_300_mrr)))\n",
        "print('precision at k = 20: {}'.format(dbow_300_pak[0]))\n",
        "print('precision at k = 40: {}'.format(dbow_300_pak[1]))\n",
        "print('precision at k = 60: {}'.format(dbow_300_pak[2]))\n",
        "print('precision at k = 80: {}'.format(dbow_300_pak[3]))\n",
        "print('precision at k = 100: {}'.format(dbow_300_pak[4]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czkqg2AlUJTl",
        "outputId": "a42deaa9-c5ea-4273-d267-040a9b455934"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DBOW with vector size 300 results:\n",
            "MRR: 0.052692182549789295\n",
            "precision at k = 20: 0.21364975380361428\n",
            "precision at k = 40: 0.27462429912750225\n",
            "precision at k = 60: 0.3035470857379756\n",
            "precision at k = 80: 0.3165068205315578\n",
            "precision at k = 100: 0.33488324744464126\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dbow_100_mrr, dbow_100_pak = evaluate_model('./Models/PV_DBOW_vec100', [20, 40, 60, 80, 100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCMnC2kVN7-I",
        "outputId": "07b9b4d5-21d9-4cb7-8ac0-cd9395877b64"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 99/99 [09:09<00:00,  5.55s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('DBOW with vector size 100 results:')\n",
        "print('MRR: {}'.format(np.mean(dbow_100_mrr)))\n",
        "print('precision at k = 20: {}'.format(dbow_100_pak[0]))\n",
        "print('precision at k = 40: {}'.format(dbow_100_pak[1]))\n",
        "print('precision at k = 60: {}'.format(dbow_100_pak[2]))\n",
        "print('precision at k = 80: {}'.format(dbow_100_pak[3]))\n",
        "print('precision at k = 100: {}'.format(dbow_100_pak[4]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZpUXtrnjULS2",
        "outputId": "9bfb7691-3241-4491-e941-816e5c1e7c80"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DBOW with vector size 100 results:\n",
            "MRR: 0.026599293137089478\n",
            "precision at k = 20: 0.13029803053374678\n",
            "precision at k = 40: 0.1600316454576327\n",
            "precision at k = 60: 0.17870817278157067\n",
            "precision at k = 80: 0.19168114112736995\n",
            "precision at k = 100: 0.2103352911278365\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dm_300_mrr, dm_300_pak = evaluate_model('./Models/PV_DM_vec300', [20, 40, 60, 80, 100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbM9O5CdPNFX",
        "outputId": "2e5f90a7-9362-494f-f2df-1a487425cf27"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 99/99 [11:47<00:00,  7.15s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('DM with vector size 300 results:')\n",
        "print('MRR: {}'.format(np.mean(dm_300_mrr)))\n",
        "print('precision at k = 20: {}'.format(dm_300_pak[0]))\n",
        "print('precision at k = 40: {}'.format(dm_300_pak[1]))\n",
        "print('precision at k = 60: {}'.format(dm_300_pak[2]))\n",
        "print('precision at k = 80: {}'.format(dm_300_pak[3]))\n",
        "print('precision at k = 100: {}'.format(dm_300_pak[4]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5t4XqMAV5oB",
        "outputId": "7f858062-70d3-493f-da2b-4144e53bf219"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DM with vector size 300 results:\n",
            "MRR: 0.018768259720678276\n",
            "precision at k = 20: 0.04359157662586688\n",
            "precision at k = 40: 0.05194164071797719\n",
            "precision at k = 60: 0.05836116298139203\n",
            "precision at k = 80: 0.06448413370253436\n",
            "precision at k = 100: 0.06722188208559116\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dm_100_mrr, dm_100_pak = evaluate_model('./Models/PV_DM_vec100', [20, 40, 60, 80, 100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "toZHJpONTYkl",
        "outputId": "a5a7e140-9d05-42fe-b746-90621ccb1a3a"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 99/99 [09:20<00:00,  5.67s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('DM with vector size 100 results:')\n",
        "print('MRR: {}'.format(np.mean(dm_100_mrr)))\n",
        "print('precision at k = 20: {}'.format(dm_100_pak[0]))\n",
        "print('precision at k = 40: {}'.format(dm_100_pak[1]))\n",
        "print('precision at k = 60: {}'.format(dm_100_pak[2]))\n",
        "print('precision at k = 80: {}'.format(dm_100_pak[3]))\n",
        "print('precision at k = 100: {}'.format(dm_100_pak[4]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4wZDR1jV8i1",
        "outputId": "3e7869a2-9490-44cf-d1ad-cdb48deefe54"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DM with vector size 100 results:\n",
            "MRR: 0.0002619389159030986\n",
            "precision at k = 20: 0.000505050505050505\n",
            "precision at k = 40: 0.0002525252525252525\n",
            "precision at k = 60: 0.0002463661000246366\n",
            "precision at k = 80: 0.0002463661000246366\n",
            "precision at k = 100: 0.0002463661000246366\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "d6gszVjiV-Up"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}