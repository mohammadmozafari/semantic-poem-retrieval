{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Install Required Packages"
      ],
      "metadata": {
        "id": "GS9eY8URx0eE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install hazm\n",
        "!pip install tokenizers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DlPQnXxzbPfi",
        "outputId": "e774388d-aab9-4e96-dfdf-320a2474e48b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting hazm\n",
            "  Downloading hazm-0.7.0-py3-none-any.whl (316 kB)\n",
            "\u001b[?25l\r\u001b[K     |█                               | 10 kB 18.9 MB/s eta 0:00:01\r\u001b[K     |██                              | 20 kB 22.0 MB/s eta 0:00:01\r\u001b[K     |███                             | 30 kB 24.2 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 40 kB 19.2 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 51 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 61 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 71 kB 9.3 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 81 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 92 kB 10.8 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 102 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 112 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 122 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 133 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 143 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 153 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 163 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 174 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 184 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 194 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 204 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 215 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 225 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 235 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 245 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 256 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 266 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 276 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 286 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 296 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 307 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 316 kB 9.7 MB/s \n",
            "\u001b[?25hCollecting libwapiti>=0.2.1\n",
            "  Downloading libwapiti-0.2.1.tar.gz (233 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▍                              | 10 kB 29.3 MB/s eta 0:00:01\r\u001b[K     |██▉                             | 20 kB 24.1 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 30 kB 29.3 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 40 kB 33.8 MB/s eta 0:00:01\r\u001b[K     |███████                         | 51 kB 36.8 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 61 kB 38.1 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 71 kB 38.6 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 81 kB 39.2 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 92 kB 40.7 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 102 kB 42.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 112 kB 42.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 122 kB 42.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 133 kB 42.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 143 kB 42.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 153 kB 42.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 163 kB 42.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 174 kB 42.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 184 kB 42.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 194 kB 42.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 204 kB 42.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 215 kB 42.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 225 kB 42.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 233 kB 42.2 MB/s \n",
            "\u001b[?25hCollecting nltk==3.3\n",
            "  Downloading nltk-3.3.0.zip (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 32.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.3->hazm) (1.15.0)\n",
            "Building wheels for collected packages: nltk, libwapiti\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.3-py3-none-any.whl size=1394485 sha256=56de3de1eaa607319e61fd3b5b026d648c399bde207f56cf6e1d55b8b5152b89\n",
            "  Stored in directory: /root/.cache/pip/wheels/9b/fd/0c/d92302c876e5de87ebd7fc0979d82edb93e2d8d768bf71fac4\n",
            "  Building wheel for libwapiti (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for libwapiti: filename=libwapiti-0.2.1-cp37-cp37m-linux_x86_64.whl size=155068 sha256=8e45a6b87b2958ce95afa20e20b0f6300a462dffd51af90e98ce2920d382bbde\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/b2/5b/0fe4b8f5c0e65341e8ea7bb3f4a6ebabfe8b1ac31322392dbf\n",
            "Successfully built nltk libwapiti\n",
            "Installing collected packages: nltk, libwapiti, hazm\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed hazm-0.7.0 libwapiti-0.2.1 nltk-3.3\n",
            "Collecting tokenizers\n",
            "  Downloading tokenizers-0.11.5-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8 MB 8.8 MB/s \n",
            "\u001b[?25hInstalling collected packages: tokenizers\n",
            "Successfully installed tokenizers-0.11.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Required Packages"
      ],
      "metadata": {
        "id": "xLE4NBFax6qJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": true,
        "id": "Q6DhynlWa6dn"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "from hazm import *\n",
        "import codecs\n",
        "import tqdm\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import yaml\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers, processors\n",
        "from gensim.models import FastText\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "\n",
        "normalizer = Normalizer()\n",
        "stemmer = Stemmer()\n",
        "lemmatizer = Lemmatizer()\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read data"
      ],
      "metadata": {
        "id": "6AQzubT6yCV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1-3GUGSQ27nG2LZzx6iOGyOpXGSH5HAED' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1-3GUGSQ27nG2LZzx6iOGyOpXGSH5HAED\" -O poems.zip && rm -rf /tmp/cookies.txt\n",
        "!unzip poems.zip\n",
        "!mkdir Models"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvC3jbBKa0CA",
        "outputId": "acce17d2-80dc-4636-e5f4-af23352784e2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-02-18 20:12:08--  https://docs.google.com/uc?export=download&confirm=&id=1-3GUGSQ27nG2LZzx6iOGyOpXGSH5HAED\n",
            "Resolving docs.google.com (docs.google.com)... 74.125.142.100, 74.125.142.102, 74.125.142.101, ...\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.142.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-08-84-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/nktnd49ftgshj4sfb5rq68vc8b92hc79/1645215075000/07093824109256912489/*/1-3GUGSQ27nG2LZzx6iOGyOpXGSH5HAED?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2022-02-18 20:12:09--  https://doc-08-84-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/nktnd49ftgshj4sfb5rq68vc8b92hc79/1645215075000/07093824109256912489/*/1-3GUGSQ27nG2LZzx6iOGyOpXGSH5HAED?e=download\n",
            "Resolving doc-08-84-docs.googleusercontent.com (doc-08-84-docs.googleusercontent.com)... 74.125.142.132, 2607:f8b0:400e:c08::84\n",
            "Connecting to doc-08-84-docs.googleusercontent.com (doc-08-84-docs.googleusercontent.com)|74.125.142.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 18074535 (17M) [application/zip]\n",
            "Saving to: ‘poems.zip’\n",
            "\n",
            "poems.zip           100%[===================>]  17.24M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2022-02-18 20:12:09 (273 MB/s) - ‘poems.zip’ saved [18074535/18074535]\n",
            "\n",
            "Archive:  poems.zip\n",
            "replace abusaeed_norm.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: abusaeed_norm.txt       \n",
            "  inflating: amir_norm.txt           \n",
            "  inflating: anvari_norm.txt         \n",
            "  inflating: asadi_norm.txt          \n",
            "  inflating: asad_norm.txt           \n",
            "  inflating: attar_norm.txt          \n",
            "  inflating: babaafzal_norm.txt      \n",
            "  inflating: bahaee_norm.txt         \n",
            "  inflating: bahar_norm.txt          \n",
            "  inflating: bidel_norm.txt          \n",
            "  inflating: eraghi_norm.txt         \n",
            "  inflating: farrokhi_norm.txt       \n",
            "  inflating: ferdousi_norm.txt       \n",
            "  inflating: feyz_norm.txt           \n",
            "  inflating: ghaani_norm.txt         \n",
            "  inflating: gilani_norm.txt         \n",
            "  inflating: hafez_norm.txt          \n",
            "  inflating: hatef_norm.txt          \n",
            "  inflating: helali_norm.txt         \n",
            "  inflating: iqbal_norm.txt          \n",
            "  inflating: jami_norm.txt           \n",
            "  inflating: kamal_norm.txt          \n",
            "  inflating: khaghani_norm.txt       \n",
            "  inflating: khajoo_norm.txt         \n",
            "  inflating: khayyam_norm.txt        \n",
            "  inflating: khosro_norm.txt         \n",
            "  inflating: manoochehri_norm.txt    \n",
            "  inflating: moulavi_norm.txt        \n",
            "  inflating: naserkhosro_norm.txt    \n",
            "  inflating: nezari_norm.txt         \n",
            "  inflating: obeyd_norm.txt          \n",
            "  inflating: onsori_norm.txt         \n",
            "  inflating: orfi_norm.txt           \n",
            "  inflating: ouhadi_norm.txt         \n",
            "  inflating: parvin_norm.txt         \n",
            "  inflating: rahi_norm.txt           \n",
            "  inflating: razi_norm.txt           \n",
            "  inflating: roodaki_norm.txt        \n",
            "  inflating: saadi_norm.txt          \n",
            "  inflating: saeb_norm.txt           \n",
            "  inflating: salman_norm.txt         \n",
            "  inflating: sanaee_norm.txt         \n",
            "  inflating: seyf_norm.txt           \n",
            "  inflating: shabestari_norm.txt     \n",
            "  inflating: shahnematollah_norm.txt  \n",
            "  inflating: shahriar_norm.txt       \n",
            "  inflating: vahshi_norm.txt         \n",
            "  inflating: zahir_norm.txt          \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [],
      "source": [
        "poets = glob.glob('./*.txt')\n",
        "poems = []\n",
        "original_poems = []\n",
        "\n",
        "for poem_file in poets:\n",
        "    with open(poem_file, encoding='utf-8', mode='r') as fp:\n",
        "        line = fp.readline()\n",
        "        cnt = 1\n",
        "        box = ''\n",
        "        while line:\n",
        "            if line.strip() != '':\n",
        "                box = box + '\\n' + line.strip()\n",
        "                if cnt % 2 == 0:\n",
        "                    original_poems.append(box)\n",
        "                    tokens = word_tokenize(box)\n",
        "                    refined_tokens = []\n",
        "                    for token in tokens:\n",
        "                        refined_tokens.append(token)\n",
        "                    poems.append(' '.join(refined_tokens))\n",
        "                    box = ''\n",
        "                cnt += 1\n",
        "            line = fp.readline()"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "ySucN75Aa6dr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(poems)]"
      ],
      "metadata": {
        "id": "dNBN1TOFeIHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Doc2Vec"
      ],
      "metadata": {
        "id": "e83WBlVYyHga"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PV-DM, Vector_size = 100"
      ],
      "metadata": {
        "id": "K3ilfRD8yKhp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_epochs = 20\n",
        "vec_size = 100\n",
        "alpha = 0.025\n",
        "\n",
        "model = Doc2Vec(size=vec_size,\n",
        "                alpha=alpha, \n",
        "                min_alpha=0.00025,\n",
        "                min_count=1,\n",
        "                dm =1)\n",
        "  \n",
        "model.build_vocab(tagged_data)\n",
        "\n",
        "for epoch in tqdm.tqdm(range(max_epochs)):\n",
        "    model.train(tagged_data,\n",
        "                total_examples=model.corpus_count,\n",
        "                epochs=model.iter)\n",
        "    model.alpha -= 0.0002"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmZ934btfNdj",
        "outputId": "4d32f1d7-4fbe-49c3-880f-30f8db83b22f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gensim/models/doc2vec.py:570: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
            "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n",
            "  0%|          | 0/20 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:17: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  5%|▌         | 1/20 [06:45<2:08:27, 405.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|█         | 2/20 [13:24<2:00:31, 401.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 15%|█▌        | 3/20 [20:03<1:53:26, 400.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 4/20 [26:50<1:47:27, 402.94s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 25%|██▌       | 5/20 [33:30<1:40:31, 402.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|███       | 6/20 [39:59<1:32:43, 397.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 35%|███▌      | 7/20 [46:25<1:25:18, 393.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 8/20 [52:52<1:18:18, 391.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 45%|████▌     | 9/20 [59:21<1:11:38, 390.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 10/20 [1:05:53<1:05:12, 391.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 55%|█████▌    | 11/20 [1:12:25<58:44, 391.56s/it]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 12/20 [1:19:00<52:20, 392.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 65%|██████▌   | 13/20 [1:25:35<45:51, 393.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|███████   | 14/20 [1:32:11<39:25, 394.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 75%|███████▌  | 15/20 [1:38:50<32:57, 395.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 16/20 [1:45:25<26:21, 395.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 85%|████████▌ | 17/20 [1:52:03<19:48, 396.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 17\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|█████████ | 18/20 [1:58:39<13:12, 396.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 18\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 95%|█████████▌| 19/20 [2:05:16<06:36, 396.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [2:11:56<00:00, 395.84s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('./Models/PV_DM_vec100')"
      ],
      "metadata": {
        "id": "x0Xaj6wY34RK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PV-DM, Vector_size = 300"
      ],
      "metadata": {
        "id": "T3N4lbqO7Rxb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_epochs = 20\n",
        "vec_size = 300\n",
        "alpha = 0.025\n",
        "\n",
        "model = Doc2Vec(size=vec_size,\n",
        "                alpha=alpha, \n",
        "                min_alpha=0.00025,\n",
        "                min_count=1,\n",
        "                dm =1)\n",
        "  \n",
        "model.build_vocab(tagged_data)\n",
        "\n",
        "for epoch in tqdm.tqdm(range(max_epochs)):\n",
        "    model.train(tagged_data,\n",
        "                total_examples=model.corpus_count,\n",
        "                epochs=model.iter)\n",
        "    model.alpha -= 0.0002"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bM6KBQXrQCeN",
        "outputId": "ab8c1c71-5a9e-4318-f1d0-7366bf65a040"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gensim/models/doc2vec.py:570: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
            "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n",
            "  0%|          | 0/20 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:16: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
            "  app.launch_new_instance()\n",
            "100%|██████████| 20/20 [2:18:36<00:00, 415.83s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('./Models/PV_DM_vec300')"
      ],
      "metadata": {
        "id": "wUoBjwhjtcAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PV-DBOW, Vector_size = 100"
      ],
      "metadata": {
        "id": "6fLy9yODkJaC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_epochs = 20\n",
        "vec_size = 100\n",
        "alpha = 0.025\n",
        "\n",
        "model = Doc2Vec(size=vec_size,\n",
        "                alpha=alpha, \n",
        "                min_alpha=0.00025,\n",
        "                min_count=1,\n",
        "                dm = 0)\n",
        "  \n",
        "model.build_vocab(tagged_data)\n",
        "\n",
        "for epoch in tqdm.tqdm(range(max_epochs)):\n",
        "    model.train(tagged_data,\n",
        "                total_examples=model.corpus_count,\n",
        "                epochs=model.iter)\n",
        "    model.alpha -= 0.0002"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YS1rpQwFkJiZ",
        "outputId": "df1b7cbd-ed2a-441b-8143-c86f045e18bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gensim/models/doc2vec.py:570: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
            "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n",
            "  0%|          | 0/20 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:16: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
            "  app.launch_new_instance()\n",
            "100%|██████████| 20/20 [1:12:52<00:00, 218.61s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('./Models/PV_DBOW_vec100')"
      ],
      "metadata": {
        "id": "qY78vfkH91Ge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PV-DBOW, Vector_size = 300"
      ],
      "metadata": {
        "id": "GxqEMVsy7NSD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_epochs = 20\n",
        "vec_size = 300\n",
        "alpha = 0.025\n",
        "\n",
        "model = Doc2Vec(size=vec_size,\n",
        "                alpha=alpha, \n",
        "                min_alpha=0.00025,\n",
        "                min_count=1,\n",
        "                dm = 0)\n",
        "  \n",
        "model.build_vocab(tagged_data)\n",
        "\n",
        "for epoch in tqdm.tqdm(range(max_epochs)):\n",
        "    model.train(tagged_data,\n",
        "                total_examples=model.corpus_count,\n",
        "                epochs=model.iter)\n",
        "    model.alpha -= 0.0002"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QeDN_OKX94ay",
        "outputId": "21df37f3-fd75-41fd-8b15-7b0e0b9fde85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gensim/models/doc2vec.py:570: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
            "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n",
            "  0%|          | 0/20 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:16: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
            "  app.launch_new_instance()\n",
            "100%|██████████| 20/20 [1:20:21<00:00, 241.06s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('./Models/PV_DBOW_vec300')"
      ],
      "metadata": {
        "id": "ABn3oTfwMUtB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Zip saved models"
      ],
      "metadata": {
        "id": "lqLsAmR5d_lH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r doc2vec.zip ./Models"
      ],
      "metadata": {
        "id": "ML0ceFlNygaq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c12e6c1-6026-48ef-e00f-a5518dd226c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: Models/ (stored 0%)\n",
            "  adding: Models/PV_DM_vec300.wv.vectors.npy (deflated 7%)\n",
            "  adding: Models/PV_DM_vec300.docvecs.vectors_docs.npy (deflated 7%)\n",
            "  adding: Models/PV_DM_vec300.trainables.syn1neg.npy (deflated 7%)\n",
            "  adding: Models/PV_DM_vec300 (deflated 75%)\n",
            "  adding: Models/PV_DM_vec100.wv.vectors.npy (deflated 7%)\n",
            "  adding: Models/PV_DM_vec100.trainables.syn1neg.npy (deflated 7%)\n",
            "  adding: Models/PV_DM_vec100.docvecs.vectors_docs.npy (deflated 7%)\n",
            "  adding: Models/PV_DM_vec100 (deflated 75%)\n",
            "  adding: Models/PV_DBOW_vec300.wv.vectors.npy (deflated 9%)\n",
            "  adding: Models/PV_DBOW_vec300.trainables.syn1neg.npy (deflated 7%)\n",
            "  adding: Models/PV_DBOW_vec300.docvecs.vectors_docs.npy (deflated 7%)\n",
            "  adding: Models/PV_DBOW_vec300 (deflated 75%)\n",
            "  adding: Models/PV_DBOW_vec100 (deflated 75%)\n",
            "  adding: Models/PV_DBOW_vec100.wv.vectors.npy (deflated 8%)\n",
            "  adding: Models/PV_DBOW_vec100.trainables.syn1neg.npy (deflated 7%)\n",
            "  adding: Models/PV_DBOW_vec100.docvecs.vectors_docs.npy (deflated 7%)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "pycharm-d7999b95",
      "language": "python",
      "display_name": "PyCharm (deep_hw2_q5)"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "name": "NLP-Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}